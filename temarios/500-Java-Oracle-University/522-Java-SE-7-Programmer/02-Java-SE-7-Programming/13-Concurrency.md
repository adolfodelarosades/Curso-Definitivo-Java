# 13: Concurrency

1. Concurrency, Part 1 - 21m
2. Concurrency, Part 2 - 10m
3. Concurrency, Part 3 - 22m
4. Practice 13-1: Using the java.util.concurrent Package - 11m
5. Practice 13-2: Using the Fork-Join Framework - 10m

## 1. Concurrency, Part 1 - 21m

So after we, of course, looked at the practice labs for lesson 12, let's go ahead now and look at lesson 13, which covers concurrence. This lesson is going to talk about the use of atomic variables. It's going to look about the concept of the usage of the ReentrantReadWriteLock. We're going to look at the java.util.concurrent collections, describe the synchronized classes, and, of course, use an ExecutorService to concurrently execute tasks, and apply the Fork-Join framework, which, by the way, is part of the Java SE 7. It's a new feature.

So the java.util.concurrent package was introduced in Java 5, Java SE 5, which contains, by the way, classes that are useful in concurrent programming. Again, these features include concurrent collections, synchronization and login alternatives, rather than using the synchronized block, and thread poolings. Again, we have thread pools, fixed and dynamic thread count pools available. And, of course, the parallel and divide conquer Fork-Join, which by the way, is a new feature in the SE 7.

Now looking at the java.util.concurrent.atomic package. Well, this atomic package contains classes that support the lock-free thread-safe programming on single variables. We see an example here, where in this case, we create a new instance of atomic integer, which is, again in this case, set to 5. And then down here, we'll go ahead and we say if compare and set 5 to 42, go ahead then of course in this case, print, replace 5 with 42. Again, an atomic operation ensures that the current value is 5 and then set it to 42. On CPU architectures that support a native compare and set operations that there will be no need for login when executing this example shown here. Older architectures may require some form of internal login.

Continuing with the concurrency, now we're going to look at the locks package. The java.util.concurrent.locks package is a framework for locking and waiting for conditions that is again distinct from built-in synchronization and monitors. So here we have an example, shopping cart, and we go ahead and create a new instance of the reentrant read write lock, reference variable-- again, this would be a single-writer, multi-reader lock.

And then within the method add item that takes an object, we'll go ahead and, in this case, lock it for write. And again, do modify your shopping cart, and of course, after that, go ahead and lock. So you use the lock to modify whatever you need to modify, and this lock method is invoked on the write lock method, which is invoked on the reference variable of the reentrant read write lock. And then after you've done, you go ahead and unlock that.

So one of the features of the java.util.concurrent.locks package is an implementation of a multi-reader and a single-writer lock. Of course, a thread may not have-- or obtain a read lock while a write lock is in use. And multiple threads can concurrently acquire the read lock, but only one thread may acquire the write lock. Of course, the lock is reentrant, which means a thread that has already acquired the write lock, may call additional methods that obtain the write lock without a fear of blocking.

Continuing with the locks package, you see here the lock we can acquire-- we can acquire lock for reading in this case. Again, we have read the cart, and then maybe modify some s, and then, we again acquire the lock here, and then, of course, unlock it. Of course, here we have a getTotal, all read-only methods can concurrently execute. So obviously, in this case, you can go ahead and execute it as many times as you want concurrently.

So of course in this example, the whole idea behind what we're trying to show here is that all methods that are determined to be read only can add the necessary code to lock and unlock a read lock. Of course, a reentrant read-write lock allows concurrent execution of both a single read-only method and a multiple read only methods.

Now looking at thread-safe collections. Well, the java.util collections are not thread safe. To use collections in this case, in a thread-safe fashion, what we need to do is use synchronized code blocks for all access to a collection if rights are performed, or create synchronized wrapper using the library methods, such as the synchronized list that takes a list. And of course, or use the java.util.concurrent.collections. You should note that just because a collection is made thread safe, this does not make its elements thread safe. You need to again worry about that.

Here we pretty much tell you about some of the concurrent collections. We say that the ConcurrentLinkedQueue class supplies and efficient scalable thread safe non-blocking first in, first out queue. Again, five implementation in the java.util.concurrent support the extended blocking queue interface, which, by the way, defines blocking versions of put and take. These are the linked blocking queue, the array blocking queue, the synchronous queue, and the priority blocking queue, and, of course, the delayed queue.

Besides queues, again, this package supplies collection implementations designed to use in multi-threaded context. Again, we have the concurrent hash map, concurrent escape list map, concurrent escape list set, copy on write array list, copy on write array set. Again, when many threads are expected to access a given collection, a concurrent hash map is normally preferable to a synchronized hash map. And a concurrent skip list map is normally preferable to a synchronized tree map. Again, a copy on write array list is preferable to a synchronized array list when the expected number of reads and traversals greatly outnumber the number of updates to a list. Again, this is just a note for you in the future if you're making use of collections, just to see which, of course, are preferable and which are not.

We have a quiz here, and in this quiz, we say a copy write array list ensures the thread safety of any object added to the list. Is it true or false? Of course, it is false. Let's go ahead and look at the synchronizer. Speaking of the class semaphore that Das was talking about earlier. So the java.util.concurrent package provides five classes that aid common, special purpose synchronization. First one again in this case is the semaphore, and we say against, semaphore is a classic concurrent tool.

Typically what we say is that a semaphore maintains a set of permits. Again, threads try to acquire permits, and may block until other threads release permits. We also have they count down latch. This is a very simple, yet very common utility for blocking until a given number of signals, events, or conditions hold. So typically we use a count down latch, why, because it allows one or more threads to wait or block until of course completion of a countdown. And of course, after the countdown is complete, all waiting threads continue. A countdown latch cannot be reused.

We also have the next one, which is the CyclicBarrier, and this is a resettable multi-way synchronization point used in some styles of parallel programming. What we mean by that is it is created, this CyclicBarrier, created with a party count, after the number of parties-- threads in this case-- have called await on the CyclicBarrier, they will be released, of course in this case, unblocked. A CyclicBarrier can be reused in this case, in the sense, used to create a barrier, where again, in this case, after all the number of threads have called await on the CyclicBarrier, they will be released.

You also have the other class called Phaser. And Phaser, what it does, folks, it provides a more flexible form of barrier that may be used to control phase computation among multiple threads. What do we mean by that? It's pretty much a more versatile version of the CyclicBarrier. It's new, by the way, in the Java SE 7, and the whole idea behind this is parties can register and deregister over time, causing the number of threads required before advancement to change. So in the sense that this has tendency to form a flexible form of barrier that is used to control the phase computation among multiple threads that all have to finish so that they can go to the next phase.

And the last one is Exchanger. This allows two threads to exchange objects at a rendezvous point, and is useful in certain pipeline designs. It allows, pretty much, a couple of threads to swap a pair of objects, blocking until an exchange takes place. It is bi-directional and, of course, memory efficient, an alternative to the synchronous queue that we mentioned in the previous slide.

Continue with that. Let's now look at the CyclicBarrier. We want to look closely at the CyclicBarrier, and particularly, we want to see the CyclicBarrier, which is an example of the synchronizer category of classes provided by the java.util.concurrent. So in this case, we go ahead and create a barrier. Again, we pass to, in this case, the constructor. Again, two threads must wait before they can, in this case, unblock. And then we go ahead and create a new instance of the thread. And as you see here, we go ahead and do all of that. This is like what? An NMS class, good.

So we'll go ahead and create a new instance, and in it, we'll implement the run method. In the run method, we'll go ahead and print a message, before await thread one. And then we'll go ahead and invoke the await method on the barrier, which is an instance of the CyclicBarrier. And then we go ahead, of course, in this case, and print afterwards the after wait message.

So in this example, what happened, folks, if only one thread calls await on the barrier, that thread may block forever. So after a second thread calls await, any additional call to await will again block until the required number of threads is reached. So the idea here is that a CyclicBarrier contains a method, await, which pretty much can a timeout and a time unit, which will block for a specified duration, and throw a time exception if the duration is reached. That's how we can control how long you await.

Let's look at high level threading alternatives. This is a very, very useful technique and I would like to, again-- pretty much was introduced with it-- we introduced the Fork-Join framework, which is part of the SE 7. Traditional thread related APIs can be difficult to use properly. So alternatives include this executor service, which is a higher level mechanism used to execute tasks. Again, it may create and reuse thread objects for you, and it allows you to submit work and check on the results in the future.

So the Fork-Join framework, it is a specialized work-stealing ExecutorService new to the SE 7. And we'll take a look at that in a few slides. Now looking at the executor service. ExecutorService is used to execute tasks. Pretty much it eliminates the need to manually create and manage threads. Also, the tasks might be executed in parallel, depending on the ExecutorService implementation. And of course, the tasks, by the way, can be Runnable or it can be Callable.

And implementing instances can be obtained using executors. And here's, for example, where we invoke the newCacheThreadPool method on the Executors, we get an ExecutorService. What we wanted to mention here, folks, is that the behavior of an ExecutorService, typically a CacheThreadPool ExecutorService creates new threads as needed. It can reuse its threads, and of course, threads do not die after finishing their task. And of course, it terminates threads that have been idle for 60 seconds.

By the way, other types of ExecutorService implementations are available, too. And just following the idea, here is an example where in this case we can go ahead and invoke the availableProcessors to find how many processors you have. That method invoked on the getRuntime, method, which is invoked on run, that gives me the CPU count. And after that, I'll go ahead and use the new fixed thread pool method invoked on the Executors and past the CPU count, and that can get me an Executor. Again, a fixed thread pool ExecutorService contains a fixed number of threads, reuses its threads, again, if they did not die, and it queues up work until a thread is available, and could be used to avoid overworking a system with CPU intensive tasks.

Let's go on and look at Callable interface. Well, this Callable interface defines a task submitted to an ExecutorService. It's kind of similar to the nature of the Runnable interface. But the only difference is that the Callable returns a result using generics, and of course, it can throw a checked exception. So here's an example where, in this case, I have the Callable interface, and pretty much, you need to implement the call method, and it can throw an exception. And by the way, it can retire also, in this case, a generic result.

Let's go on and look at the Future. Future, also, interface is very interesting. It is used to obtain the result from a Callable's call method. OK? And let's go ahead and see this example. So here we have an ExecutorService that we created earlier in the previous slide. And we can invoke the submit method and pass Callable. This Callable, again, in this case would be the ExecutorService controls when the work is done. And of course, it returns a Future. Again, the submit may be Callable's, and within the try method, what we do is we can invoke the get method on the future instance. Again, what this get method gets you the result of the Callable's call method, and blocks if needed, until it gets it. And of course, it can catch an execution exception, or an interrupt exception, and so forth.

So in general, because the call to the Future get method will block, again, you must do one of the following, either submit all your work to the ExecutorService before calling any get method on the Future, or any get methods on the Future instance, and of course on the class, rather, this static method, or be prepared to wait for that future to obtain the result.

And of course, use a non-blocking method, such as isDone before calling the Future get. Or use, in this case, isDone finds out if, of course, the task is done. Or use, again, this get method, using a timeout and some, in this case, time unit. Which by the way, will throw a time exception if the result is not available within a certain, given time. Lots of times we use this one here, the last one, the latter one, just to again make sure that we're not stuck in a particular loop, or something wrong is happening.

Now we should not forget, folks, that we need to shut down the ExecutorService at some point in time. So shutting down an ExecutorServer is important because its threads are nondaemon threads, and will keep your JVM from shutting down. So typically, how do we do that? Well, in general, we invoke the shutdown, except that we want this shutdown to be gracefully done, folks. OK? So that we do not disturb any clients.

So how do we do that? Within the try block, we can go ahead and say, OK, shut down. But within 08 termination time, in this case, and then we pass again the value here, which is five, and they would be again time units and seconds. So if you want to wait for Callable to finish. So let's wait for the Callable to finish before we shut down the ExecutorService. Everybody's following?

And of course, we have a quiz here. It says an ExecutorServer will attempt to use all of the available CPUs in the system. That's, of course, false.

## 2. Concurrency, Part 2 - 10m

Ah, now we're going to get into the Concurrent I/O. Where sequential blocking calls execute over a longer duration of time than the concurrent blocking calls.

So looking at this diagram here that we have on the top, imagine there are different ways to measure time. Again, in this case, in the graph, in the graphic sequence of five sequence calls that we see here, to the network server will take approximately 10 seconds if each call takes two seconds, right? On the right side, what we can do is we can have all of these-- again, in the next set-- five concurrent calls call at the same time. So they call at the same time and each one of them takes about two seconds. Then what's the total, again, in this case, what's the total time to wait? It's approximately two seconds. So obviously, concurrence definitely is attractive.

And for that, let's go and look at this example. This is a single-threaded network client. Imagine we have a class called SingleThreadClientMain. And what we do in this example is we are trying to discover which vendor offers the lowest price for an item. Again, in this case, the client will communicate with ten different network servers. Each server will take approximately two seconds to look up the requested data and return it. Again, there may be additional delays introduced by the network latency and so forth.

So pretty much, if we see this example, we see that the single-threaded client must wait for every server to respond before moving to the next one, and not all calls will induce. About 20 seconds is required to retrieve all data.

Let's go ahead and look at how actually this is built. It has a main method. And we go ahead and declare a string representing the host. And then we use the for loop and we're iterating through ports between 1. 10,000 and 10,010. That's 10 ports, 10 servers.

So first, what we do is we'll go ahead and, of course, invoke the constructor RequestResponse, and pass to it host and port. So some of you might be saying, OK, where is this class, Joe? We'll look at that later on. And we get a lookup.

And then within the try block, we'll go ahead and create a new socket. Again, this is within for loop. Bypassing the host and the board number, which is the first one is 10,000 and then the next one is 10,001 and so forth. And then we create a new scanner by passing to it the getInputStream on the sock. And then, of course, after that, we'll go ahead and invoke the next method on the scanner to get the response. OK?

And of course, we print that. We print the host, the port number, and the response. And catch a NoSuchElementException or IOException if there's an exception.

So that's a single-threaded client network line. Let's go ahead now and look at a multithreaded network client. case-- This is Part 1. In this example, we have a class called MultiThreadedClientMain. And what we're trying here to do is we're trying to discover which vendor offers the lowest price for an item. Again, in this case, the client will communicate with 10 different network servers and every server will take approximately two seconds to look up the requested data and return it. Of course, there may be additional delays introduced by the network latency.

This case here, folks, the multithreaded client does not wait for every server to respond before attempting to communicate with another server. So that's why in this case, it would be about two seconds instead of 20 seconds that are required to retrieve all the data. It's going to look how we do that.

So here, we'll go ahead and look at ThreadPool used to execute the Callables. So here, look what we do. We go ahead and invoke the newCachedThreadPool on the executors to create an ExecutorService. And then we'll go ahead and create a map used to connect the request data in with the result. So that's pretty much a map called Callables. And it's of type RequestResponse, and then the object of type future-- a future object of RequestResponse.

And this would be what? A HashMap, right? Initialized with a HashMap. And after that, we'll initialize the string host with a local host. And then loop to create and submit a bunch of Callable instances by creating, again, this loop. And now, look what we do. We create the new instance of the request. And of course, we'll go ahead and create a new network line capable, Callable, and pass to it the lookup. So we see later on this network line, Callable class. And then, of course, after that, we go ahead and invoke the submit method on what? On the ExecutorService instance and pass Callable to it, this one here.

And of course, after that, we'll go ahead and-- whatever the result that comes in-- we'll go ahead and pass it with the lookup and put it where? In the Callables, which is what? Which is now our HashMap that we created here. Everybody's following? All right. Let's go ahead and continue.

This is Part 1. Let's go ahead and look at Part 2. In Part 2, we should again stop accepting new Callables by doing an es.shutdown. But you also should what? Block until all Callables have a chance to finish. By, again, invoking the awaitTermination and give it five seconds to finish. Everybody's following here? Good.

Continuing with that, now let's take a look at the multithreaded network client. So within the for loop, we'll go ahead and invoke the key set. By the way, invoking the key set on a HashMap will give you what? We return the set of keys. OK? And of course, we iterate through them using the advanced loop. And what do we do here? We'll go ahead and get-- invoke the get method on the Callables, and pass the lookup that we just-- passed the key. And then, of course, this will, whatever we get, will return for us the value, which is the future. In this, the instance of future, which is of type future RequestResponse.

And then within the try block, we'll go ahead and invoke the get method on the future. That gets me what? The lookup itself.

Now we go ahead and print it out, OK? In this case, the host and the port number and the response. And catch an exception if there's an exception. Again, this is why the Callables maps exist. Again, the future.get method fails if the task fails. Of course, in this case, we'll go ahead and print this error message. Everybody's following?

Continuing with that, let's go ahead and to look at Part 4, where in this case, remember, I mentioned to you that we would see at some point in time the request response. Here it is. It has an instance variable host of type string. Another instance variable port of type integer. And of course a string representing the response. And it has a constructor that takes a host and a port and initializes with them the host and the port. And of course, some equals and hashCode method.

Now let's look at the NetworkClientCallable. Remember that this NetworkClientCallable, we actually used it earlier. If you take a look here, this is the instance that creates a Callable for us. Remember that? So now let's go ahead and look at that. And that would be in Part 5.

This class, called NetworkClientCallable, that's the one that implements the Callable interface, making use of request response. And of course, in this case, we'll go out and declare an instance of request response, an instance variable. And of course, we have the constructor NetworkClientCallable that takes the response and uses it to initialize the lookup, which is of course the response here.

And look what we do here. In this network Callable, client Callable that implements the Callable, we, in this case, override the call method. So this call method, again, in this case, what it does is returns a request response. But look what we do inside. Inside, we'll go ahead and create a socket with the host and the port number, the current one. And we create a new instance of the scanner by again passing to it the getInputStream method that is invoked on the socket, reference variable socket. And then after that, we'll go ahead and invoke the next on the scanner instance to get the response in this case. And of course, return that lookup. OK? Initialize within the response of lookup, and, of course, return it.

Everybody's following?

## 3. Concurrency, Part 3 - 22m

Now let's look at parallelism. When modern systems contain multiple CPUs and taking advantage of the processing power in a system requires you to execute tasks in parallel on multiple CPUs. Again, the concept here is to use the divide and conquer, where a task should be divided into subtasks and you should attempt to identify those tasks that can be executed in parallel. Of course, some problems can be difficult to execute as parallel tasks. Of course, some problems are easier and of course, servers that support multiple clients can use a separate task to handle every client. We should also be aware of our hardware. Again, scheduling too many parallel tasks can negatively impact the perform, so we of course, need to worry about that. And by the way, we have a mechanism in or method in Java that we can use to find out how many processors we have. Again, this is the available processors, which is invoked on the get runtime method, which by the way, is invoked on the run time. That get me, the what? The number of processors on my hardware system. Everybody's following?

Now let's go ahead and look at the task without parallelism. When modern systems contain multiple CPUs, if you do not leverage threats in some way, only a portion of your system's processing power will be utilized. Like in this case, I have four CPUs and I'm using only one because of the fact that I have one task per CPU, in this case. And I only have that task and it's handled by this CPU and the other one of course, what they're doing, the three CPUs they are idling. That's a bad idea. So let's look at the naive parallelism. When a simple parallel solution breaks the data to be processed into multiple sets again, one data set for every CPU and one thread to process every data set. So in this case, we want to separate the data and of course, in this case, we can have all the different types of data handled by all the CPUs at the same time. And that's really what we are striving for.

So again every array in this case can share a reference to a single large array but access only a subset in a non-blocking thread safe way. So let's go ahead now and look at the need for the Fork-Join framework. This is of course, the new framework that was introduced in the SE 7. When splitting datasets into equal sides, subsets for every thread to process, has a couple of problems. Ideally all CPUs should be fully utilized until the task is finished but CPUs may run at different speed, or maybe non-Java tasks require CPU time and may reduce the time available for the Java thread to spend executing on a CPU. Of course, the data being analyzed may require a varying amount of time to process. All these could be issues, folks.

Now looking at work-stealing. Well, to keep multiple threads busy, what we do for this framework is we divide the data to be processed into a large number of subsets. Again, we assign the data subsets to a thread's processing queue and every thread will have many subsets queued. So if a thread finishes all it's subsets early, it can actually steal subsets from another thread. That's the whole idea behind that. One thing that again, we want to mention here, folks, is that we must determine the optimal size of the work to add to every thread's processing queue. And of course, overly sub-dividing the data to be processed can actually cause unnecessary overhead while insufficiently sub-dividing the data can result in an under utilization of CPUs. We've got to be careful with that, folks.

Let's go ahead and look at a single-threaded example. Here's a single-threaded example where in this case, we create an array of integers called data and we size it to what? 1G. That's a very large data set. And then we use the for loop and again, with between zero and the size, which is 1G. OK, which is 1024 times 1024 times 256. And then what do we do in this case? We'll go ahead and invoke the current method on the thread local random and on it create the next integer and of course, use it to initialize with it data I. Which is again, index that I. And we do iterate through all of that. Obviously, this is going to be a big, huge job.

So fill up the array with values and that, if you do it serially, it's going to take a long time. And then after that we'll go ahead and declare an integer max and of course, get it to initialize it with the minimum value of integer and then use the enhanced for loop again here, to sequentially search the array for the largest value. And that of course, being the largest value. So obviously you see here, there are again, in this case, a couple of tasks. The first one is to initialize the array and the second one is to search the array for the largest possible value that could be again, in this case, be done in parallel.

So for that look, folks, what are we going to do? We are going to use the Fork-Join task. The Fork-Join task object represents a task to be executed. Again, a task contains the code and the data to be processed, similar to Runnable or a Callable. Of course, a huge number of tasks are created and processed by a small number of threats in the Fork-Join pool. So the Fork-Join task typically creates more Fork-Join tasks instances and the data to be processed has been subdivided adequately. And developers typically use these subclasses, which are recursiveAction again, which is used when a task does not need to return a result. And the other one is recursiveTask, which again, when a task does need to return a result.

Continuing with the example, let's go and look at now the RecursiveTask. Well, here we are going to create a class called FindMaxTask, which extends, what? RecursiveTask. Again, here the result type of the task would be integer. We declare a variable threshold, which is by the way final, it's a type of integer. We declared an array of integer which is final. We declare a couple of private variables, stop and end, and of course after that we'll go ahead and call, we define the FindMaxTask, the constructor, which pretty much takes an array, a start, an end and a threshold. And of course, copy the parameter fields here again, to initialize that. And we also have a method compute again, where the work is done and we should notice the genetic return type in this case, integer. You see here. And here again we show this code later.

Now of course speaking of compute, here's later in the next slide. Here's the compute method. In the commute method we say, OK, if the data is small enough, this is pseudocode, by the way. If the data is small enough, process data and return the results, or else look what we do. We split the data into left and right and then we'll go ahead and create a new task, T1 with the left data and we'll go ahead and execute extra security by invoking the Fork-Join method on that task, task one. And then we'll go ahead and create a Task 2 on the right data. Again, we split the data, the array into two parts and of course, in this case, we'll go ahead and return by doing what? By combining, OK, in this case, the task, T1 and task, T2 that got computed, join them, combine them together to get the result. OK?

Let's go ahead and look at the compute example, below the threshold. Again, this is below the threshold, which means when you split the array into two. So we have a compute method, we know it returns an integer and here we say, if the end minus start is less than threshold, again in this case, you decide about the threshold, that's up to you. And we go ahead and initialize in this case, declare max in this case, as a local variable and initialize it with a minimum value of the integer. And then we use the for loop and we declare I and we start from start, and then up to the end and if we do that, over there, we go ahead and copy the array value into end and then check if end is greater than max and if it is set the end to max and keep on going.

And of course, at the end we return, what? The max value. Or else, what we need to do? Split the data and create tasks. Let's go ahead and do that. Now let's look at the above threshold. If now, the end minus start is less than the threshold then of course, in this case, we'll go ahead and find the max, or else what we do? We'll go ahead and split the array into half. OK. That's midway end minus start divided by two plus start. And then of course in this case, go ahead and create a FindMaxTask and bypassing in this case, to the constructor, FindMaxTask the array the start and the midway and the threshold.

The threshold you know, the threshold is used to do what? To find the max, right? And then of course, invoke the fork to compute that asynchronously, right? And then of course, we create a second task FindMax, too and that would contain the second half. This again, in this case, we pass myArray and then midway plus 1, again to the end, which means from the midway plus 1 to the end, so the first one is from the start of the midway and then you're going from the midway plus 1 to the end, got that? And then the threshold. And of course, what we do here, we did return the max method on the math by doing what? By passing to it the task to that compute and of course A1 dot join.

So we should notice here that the same array is passed to every task but with different start and and. Here it starts with the start and then to the midway and then this one it starts with the midway plus 1 to the end. Got that? And of course, if the subset of the values that we processed where copied into a new array each time a task was created, memory usage would quickly in this case, skyrocket. Be careful with that.

Continuing with that, let's go and now at the Fork-Join pool example. Well, the Fork-Join pool is used to execute a Fork-Join pool task and it creates a thread for every CPU in the system by default. So in general, we're going to create invoke the new Fork-Join pool to create a pool, OK? And of course, in this case, we created a task and passed to it here the data and of course, it length minus 1. And of course, the data with the length divided by 60. And of course, in this case, we invoke the-- we call the invoke method and pass to it the task's compute method is automatically called in this particular case.

Now here are some recommendations that we give you for the Fork-Join framework. We tell you, please avoid IO or blocking operations. Again, only one thread per CPU is created by default and blocking operations would keep you from utilizing all CPU resources. Also, we tell you, know your hardware, please. A Fork-Join solution will perform slower on a one-CPU system than a standard sequential solution. And of course, some CPUs increase in speed when only using a single core, potentially offsetting any performance gain provided by the Fork-Join.

And the third one is, know your problem, please. Many problems have additional overhead if executed in parallel and of course, parallel sorting for example, and that is also an issue. And by the way, if you want to look at the example, there is an example related to this framework and it's under the-- if you install the JDK1.7 it's under sample of Fork-Join and is called MergeSort. We have a quiz, also and this quiz says applying the Fork-Join framework will always result in a performance benefit. We just said it earlier. Is it true or false? False. Thank you.

So in summary what we've done here, we used the atomic variables, we used the ReentrantReadWriteLock, we made use of the concurrent collections, we again described the synchronized class and we used the ExecutorService to concurrently execute tasks, and we of course, we looked at apply the Fork-Join framework. In fact, let me just go ahead and look at an example that makes use of the, in this case, on NetBeans of the Fork-Join problem. Let me close all of this. Let me close this guy here. I don't mean these guys. And let's look at the Fork-Join. And close this one here so that I can see. So in this case, we look at the sort demo. So we have a class called SortDemo.

First, we create a new random with this long integer, we have a generate the array, this generates the array. And again, in this case, the size in this case is 100,000. And we created a method called warmup, this warmup will go ahead and create a new instance of MergeSort and pass to it the generated array using the max. Again, which is the size of the array. The MergeSort is by the way, here the MergeSort is a class and in this case, when we declare we go out and create an array, a bunch of local variables, we have a threshold, and of course we have the constructor. And this has a MergeSorting method. That takes an array, a low and a high, and of course, go through the MergeSorting. It also again, the MergeSorting, by the way, called the merge and path the middle to it. And of course, here is the merge method that, of course, in this case, will go ahead and in this case, merge pretty much the arrays.

And then we have a sort method that we call the MergeSorting. Going back to the sort demo. So we go ahead and get the current time, call the MergeSort and then and after that get current time and compute that, again this is sequentially. What we could do is if you want to look at the number of processors and then create now, in this case, the MergeSortFork. OK, by passing the number of processors. Now this is going to of course, call the MergeSortFork class, which by the way, now uses the method the Fork-Join framework. So we create the instance of the Fork-Join pool and we have a MergeSortStack that extends RecursiveAction. And class, in this case, of course, we have the MergeSortTask, in this case, a constructor, and we override the compute method. OK, here.

Well, you see here, we go ahead and we say, if my high minus low is less than or equal to threshold then go ahead and call the sort and pass the array low and high, else divide again and this case get the middle, OK? And then after that we'll go ahead and invoke the invoke all by passing a new instance of the MergeSortTask passing the array, low and middle. And then of course, in this case, a new MergeSortStack passing the array, middle and high. And of course, called the MergeMiddle. And the merge in this case, by passing the middle and merge method is here. Again, merge is the two sorted arrays again in this case, and that's exactly what it does in this case.

And of course, if you want to run the SortDemo what it's going to give you in this case, because we already saw that. First, it finds the total time in MergeSort when you do it sequentially and then after that it will go ahead and find the total times in parallel using the MergeSort. Let's go ahead and do that. And if you can view that, you can see it's taking a little bit of time. But you see how the total time taken in the MergeSort if it's done sequentially, what's the size here? The time, 4,031 milliseconds. It also computes the number of processors turned on. The machine I'm using here has four processors and by the way, the total time using the Fork-Join using all four processes now is what? 15 milliseconds. Obviously, this is a huge difference, ladies and gentlemen. There is no question about that. That this Fork-Join is really quite attractive. This is a new-- the Fork-Join framework that is introduced in SE 7 and it pretty much does an excellent job.

So you see that if you do it sequentially that it takes 4,031 milliseconds and if you use the parallelism and use the Fork-Join framework obviously, for this very short, it actually computes that in 15 milliseconds. Everybody sees that? OK. All right. Continuing again in this lecture. Now we're going to go ahead and take a look at of course, a nice in this case, a practice which is pretty much using, even though it's optional, my advice to you is to try it, is to use the Java.util.concurrent package. And the second one, which is making use of the Fork-Join framework. So you get actually to work with it.

Here In the activity diagram, if we want to look at the problems and the practices in the activity diagram, just go ahead and look at that. And that would be number 13. So you see that again the practices for lesson 13, we have the first one again which makes use of the Java.util.concurrent package. And the second one that we ask you to do is making use of the Fork-Join framework. So we'll go ahead and do the practice labs for lesson 13 and once we're done, we'll get into lecturing lesson 14. Thank you.

## 4. Practice 13-1: Using the java.util.concurrent Package - 11m

Now that we looked at the Lesson 13, which covers concurrency, where we looked at the use of atomic variables, we looked at the use of a reentrant ReadWriteLock. We looked at the java.util.concurrent collections. We looked at describing the synchronizer classes. We also looked at using an ExecutorService to concurrently execute tasks, and of course, applying the fork/join framework.

Let's go ahead and look at the practices for this lesson. And we start with Practice 1, which, by the way, is optional. But we'll still want to look at it. Again, you'll modify an existing project to use an ExecutorService from the java.util.concurrent package. Again, you will create a multi-thread networking client that will rapidly read the price of a shirt from several different servers. Again, instead of manually creating the thread, you will leverage and execute a service from the java.util.concurrent package.

So for that, we open the ExecutorService project, which, by the way, I already opened here. And of course, expand the project, again, directory. We'll run main server in the example server package by right-clicking and running that.

And we open, of course, the network client, in this case, the NetworkClientMain class. And run the Main class, in this case, by right-clicking. Then we'll create this NetworkClientCallable class in the package, add the constructor, and of course, a field to receive, and of course, store a RequestResponse reference.

So before, again, getting into the NetworkClientCallable class, let's going ahead and just, in this case, take a look at our NetworkServerMain. Here it is. Well, of course, in this case, we'll create an ExecutorService. Get a run as list, Runnables, use a For loop, declare runnable, and then, of course, create a new instance of the PriceRangeServer, which is this class here that implements runnable. It has, of course, a constructor. It has an Accept method. It has a Run method. Please go through that.

Here's run method that cause the except and the except of cause method here. There is an IOException prints "Accepting connection on port," and then get the local port. Again, while the thread is not interrupted, go ahead and accept the socket. And get a new instance of the BufferedWriter, and, of course, defer some time. And catch and interrupt execution if there is one. Here, of course, use that BufferedWriter, and of course, write the price on line 34.

So they asked us to run this. We'll go ahead and run it. So it's running now. And it says accepting, again, connections through port 10,000 to 10,009. And then, of course, the main client here, network, that's the one we want to run. Again, in this case, we get an ExecutorService.

And we create a callables here and get the local host, and then use the For loop to iterate through the loops that are the ports 10,000 through 10,009 and get a RequestResponse. That's a lookup, and then, of course, create a new NetworkClientCallable. NetworkClientCallable is here. We'll take a look at that in a minute.

And then after that, we'll invoke the Submit and pass the callable to the Submit method. And that's involved on the, of course, in this case, the ExecutorService es. And that will get us a future, and then, of course, after that, we'll go ahead and invoke a Put on the callable to put the lookup in the future, and then, of course, at some point in time, shut down that executor within Try method. We'll go ahead, in this case, and wait for the termination, call the awaitTermination that, again, block until all callables have a chance to finish.

And then, of course, use the For loop here enhanced For loop. We'll go through, again, in this case, the keySet. Invoke the keySet method on the callables. And for every RequestResponse, again, in this case, we'll go ahead and invoke the Get method past the lookup on the callables in the future. And then from the future, we'll go ahead and invoke the get together lookup and then do some print information in this case and catch an ExecutionException or InterruptException, if there is one.

So we'll run the client. After a bit, let's go ahead and just take a look at this case, the NetworkClientCallable, where we said, in this case, add a constructor and a field to receive and store RequestResponse references, implement the callable interface with a genetic type of RequestResponse. We see the call this way.

Complete the call method by using the java.net.Socket, and of course, a scanner-- java.util.Scanner-- to read the response from the server. Store the result in the RequestResponse object and return it. Again, we should note that you may want to try to use the try-with-resources statement, which is part of the Java SE 7, to ensure that the Socket and Scanner objects are closed automatically.

And that's pretty much what we have here. So looking at the callable, so here's the Callable class that implements Callable. So we declare a lookup of that RequestResponse, reference variable lookup of type RequestResponse. And here, again, in this we have the constructor NetworkClientCallable that takes the lookup to initialize that. And here, we override the call method that creates the ITC as Socket here with the host and the port number, and of course, get a new instance of the Scanner, and of course, use the next method on the Scanner to get the response, and as you see, return that lookup.

So as you see here, we use the try-with-resources so that automatically, the Socket, and of course, in this case, the Scanner, are closed automatically when they're done. And of course, in the main method of the network client to query the servers currently by using the ExecutorService where they have seen that coming out the content of the main method, obtain an instance of the ExecutorService that uses a pool of cached threads.

Create a map that will be used to tie a request to the future response, like we've seen here, and code a lookup that will create a NetworkClientCallable for every network request. Again, the server should be running on local host 10,000 through 10,009. We already have seen this. And we see the output here.

And then, of course, after that, we shut down that executor. So let's just take a look at, first of all, what we looked earlier. So as you see, in the NetworkClientMain, so as it starts here, so get an instance of the ExecutorService, and then after that, we have an instance of the callables, which is a hash map. And then we get the host. Use the For loop to iterate the report numbers. And in this case, get an instance of the RequestResponse lookup and create an instance of the NetworkClientCallable by passing the lookup to it for initialization.

And then, of course, now that's the new constructor. And then invoke the Submit method on the ExecutorService and passing the callable to it. That will get us a future. And then after that, we'll go ahead and invoke the Put method on the callables and pass the lookup in the future.

Here's where we stop accepting new callables. And then, within the Try, we'll call the awaitTermination, again, block until all callables have a chance to finish. And then, of course, we use, as you see here, the enhanced For loop to iterate through the callables and get the lookup. And from it, we invoke the Get method on the callables to get the future. And then from the future, we invoke the Get to get the lookup. And then after that, we'll print the information.

Of course, they want us to run the client now by right-clicking that file and run it. Let's go ahead and do that, since we already know that the server is already running. So let's go ahead and run now the client. And as you see, it will, of course, contact the server. And then we see here, we get the information on port to 10,000 through 10,009. We get all the, again, in this case, the prices. And that pretty much ends the Practice 1 of Lesson 13. Thank you.

## 5. Practice 13-2: Using the Fork-Join Framework - 10m

Now that we looked at the Lesson 13, which covers Concurrency, where we looked at the use of atomic variables. We looked out the use of reenter and read, write, lock. We looked at the Java.UT.Concurrent collections. We looked at describing the synchronizer classes. We also looked at using an executory service to concurrently execute tasks. And, of course, applying the Fork-Join framework.

Let's go ahead and look at the practices for this lesson. And we'll start with practice one. Which, by the way, is optional but we still want to look at it. Can you modify an existing project to use and execute a service from the Java.UT.Concurrent package?

Again, you will create a multi-thread networking client that will rapidly read the price of a shirt from several different servers. Again, instead of manually creating a thread, you will leverage, and execute, a service from the Java.UT.Concurrent package.

So with that, we open the project execute or service project. Which, by the way, already opened here. And, of course, expand the project again directory. We are on the main server in the example server package by right clicking and running that.

And we open, of course, the network client in this case. The network client main class. I run the main class in this case by right clicking. Again, create this network client callable class in the package. Add the constructor and of course a [? fail ?] to receive. And of course a store and request response reference.

So before again getting into the network callable class, let's go ahead and just, in this case, take a look at our network server main. Here it is. Well of course in this case we'll create and execute those or execute a service. Get a runners list. Runnables, use for loop, declare runnable, and then of course create a new instance of the price [INAUDIBLE] server. Which is this class here that implements runnable.

It has, of course, a constructory has an accept method. It has a run method. Please go through that. It's a run method that calls the accept. In the accept of course method here there was an IO exception. Prints accepting connection on port. And then get the local port.

Again, while thread is not interrupted go ahead and accept the socket. And get a new instance of the buffer writer, and of course [INAUDIBLE] for some time. And catch and interrupt execution. There is one. Here, of course, use that buffer writer. And of course, write the price on line 34.

So they asked us to run this. We'll go ahead and run it. So it's running now. And it says, accepting again connections through port 10,000 through 10,009. And then of course the main client here, that's the one that we want to run. Again, in this case, we get an execute of service. And we create a [INAUDIBLE] here.

Then get the local host. And then use the full loop to reiterate through the loops that are the reports 10,000 through 10,009. And get a request response. That's a look up. And then, of course, create a new network client callable. Network client callable is here. We take a look at that in a minute.

So and then after that we'll invoke the subnet and pass the callable to the subnet method. And that's invoked on the, of course in this case, the executor service ES. And that will get us a future.

And then, of course, after that we'll go ahead and invoke a put on the callable to put the look up in the future. And then of course, at some point in time, shut down that executor within the try method. We'll go ahead in this case and wait for the termination. Call it wait termination [? that ?] again block until all callables have a chance to finish.

And then of course, use the full loop here and enhance full loop. We'll go through again, in this case, the key set. Invoke the key set method on the callables and for every request response. And in this case we'll go ahead and invoke the get method past the look up on the callables return the future.

And then from the future, we'll go ahead and remove the get to get the look up. And then do some print information in this case. And catch an execution exception or interrupt exception if there is one. So we'll run the client. And after a bit let's go ahead and just take a look at this case the network client callable. Where we said in this case add a constructor.

And if fail to receive and store request response references implement the callable interface with a generic type of request response. We see the code this way. Complete the call method by using the java.net.socket and, of course, a scanner-- java.ut.scanner-- to read the response from the server.

Store the result in the request response object, and return it again. You should note that you may want to try to use the try with resources statement, which is part of the javaSE7, to ensure that the socket and scanner objects are closed automatically. And that's pretty much what we have here.

So looking at the callable. So here's the callable in the class that implements callable. So we declare a look up of the request response. A reference variable look up of type of request response. And here, again, in this we have the constructor network client callable that takes a look up to initialize that.

And here, we override the core method that creates, like you see, a socket here with the host in the port number. And of course, get a new instance of the scanner. And of course, use the next method on the scanner to get the response. And as you see, return that look up.

So as you see here, we used the try with resources so that automatically the socket-- and of course, in this case, the scanner-- are closed automatically when they're done. And of course, in the main method of the network client to query the servers currently by using the executor service. Where they have seen that coming out the content of the main method.

Obtain an instance of the executor service that uses a pool of cache threads. Create a map that would be used to tie a request to a future response like we've seen here. And code a look up that will create a network client callable for every network west.

Again, the server should be running on local host 10,000 through 10,009. We already have seen this. OK, and we see the output here. And then, of course, after that we should [INAUDIBLE] back to executors. So let's just take a look at, first of all, what we looked at earlier.

So as you see, in the network client main, so as it starts here. So get an instance of the executor service. And then after that, we have an instance of the callables which is a hash map. And then we'll give the host. Use the full loop to reiterate the report numbers.

And in this case, get an instance of the request response look up and create an instance of the network client callable by bypassing the look up to it for [INAUDIBLE]. And then, of course, now that's to [INAUDIBLE]. And then invoke the subnet method on the executor service and pass in the callable to it. That will get us a future.

And then after that, we'll go ahead and invoke the put method on the callables and pass the look up in the future. Here's where we stop accepting new callables. And then within the try, we'll call the wait termination. Again, block until all callables have a chance to finish. And then of course, we use as you see here the enhanced full loop to reiterate through the callables and get the look up.

And from it, we invoke the get method on the callables to get the future. And then from the future, we invoke the get to get the look up. And then after that, we'll print the information.

Of course, they want us to run the client now by right clicking that. Fine, I'll run it. Let's go ahead and do that since we already know that the servers already running. So let's go ahead and run now the client.

And as you see, it will of course contact the server. And then we see here, we get the information on port 10,000 through 10,009. We get all, again in this case, the prices. And that pretty much ends the practice one of lesson 13. Thank you.
